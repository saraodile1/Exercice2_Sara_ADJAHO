{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install wikipedia-api\n",
    "%pip install google-api-python-client\n",
    "%pip install praw\n",
    "%pip install googlesearch-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "from googleapiclient.discovery import build\n",
    "import praw\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre du produit: Chargeur iphone Rapide, certifié Apple MFi 20W Chargeur Rapide USB c pour iphone avec 2m Câble USB C pour Apple iPhone 14/14Pro/13/13 Mini/13 Pro/13 Pro Max/12/12 Pro/12 Pro Max/11/11 Pro/11 Pro Max\n",
      "Prix du produit: 12, \n",
      "Avis sur le produit: 4,2 sur 5 étoiles\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.amazon.fr/dp/B0BCFJPLT7'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find(id='productTitle').get_text(strip=True)\n",
    "    price = soup.find('span', {'class': 'a-price-whole'}).get_text(strip=True)\n",
    "    avis = soup.find(class_=\"a-icon-alt\").get_text()\n",
    "    \n",
    "    print(f'Titre du produit: {title}')\n",
    "    print(f'Prix du produit: {price} ')\n",
    "    print(f'Avis sur le produit: {avis}')\n",
    "else:\n",
    "    print('La requête a échoué avec le statut:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Date : 2025-03-26T22:23:21.000Z\n",
      "📝 Tweet : @kufeogluu ML Data science merakı olan arkadaşlar birçok şirket uygulamalarına bunları entegre etmeye çalışıyor çağa ayak uydurma diyelim bu boşluktan faydalanıp staj kovalayabilirsiniz herkese başarılar\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:22:46.000Z\n",
      "📝 Tweet : @janusZer0 @StaatSozialist It's 2025. You would be better off with individuals with degrees/expertise in Computer Science, Data Science, Cybersecurity, Law, and Marketing/Communications, etc., and you might want someone with skills in actual Military Intelligence.\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:22:07.000Z\n",
      "📝 Tweet : @JoshHoltTEN Here is science: fit to 2020-2023 sea level data shows CONSTANT 3.7 mm/yr = 0.14 in/yr rate, see plot. \n",
      "No signs of acceleration. Nothing like \"0.3-0.4\" in/yr. \n",
      "Science is: fit the data, not pick and choose the data to fit one's prejudices. \n",
      "(Data: https://t.co/MeCJcraNAU) https://t.co/1SYzCgij3I\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:21:07.000Z\n",
      "📝 Tweet : Just took a class on poster design…\n",
      "Turns out, science CAN be beautiful. Who knew? 😎🎨\n",
      "\n",
      "The MeshAlyzer team is now equipped to make our data not just smart — but stunning. Stay tuned for the glow-up. 💡📊💅\n",
      "\n",
      "#MeshAlyzer #PosterPower #grind #herniafriends #SciComm #DesignMatters\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:19:14.000Z\n",
      "📝 Tweet : @DefiantLs You have the location, time of day, and I bet other security cameras.  Also why can't the FBI or Police supeona phone tower data.  I am sure she has a cell phone on her.  See who was at that time and give em a call or stop by where they live or work.  This isn't rocket science...\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:16:55.000Z\n",
      "📝 Tweet : 4/ We'll increasingly use mix of data science/econometric modelling/AI to effectively map on/offline interactions and assign credit to elements of marketing mix more accurately. Challenge for communicators is integrating credit for earned media/awareness into this approach.\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:15:55.000Z\n",
      "📝 Tweet : At least it would be easy, you guys arent able to research court docs or understand data, physics &amp; science.\n",
      "#freekarenread https://t.co/djdXldW16D\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:15:36.000Z\n",
      "📝 Tweet : @KevKeyboard @mtaimemes @atrupar That’s not how science works at all.🤷 A theory is true or not true. Changing with with new data is another way of saying your theory is wrong.\n",
      "❤️ Likes : 0\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:15:18.000Z\n",
      "📝 Tweet : @lazarbian this is one of the reasons i got a degree in data science\n",
      "\n",
      "let me sit at a computer all day !!!\n",
      "❤️ Likes : 1\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n",
      "📅 Date : 2025-03-26T22:12:28.000Z\n",
      "📝 Tweet : @Bushra1Shaikh They've done nothing because they haven't evolved out of the barbaric, draconian 6th century. \n",
      "You fools believe in revelations, and the civilized world believes in data, science, evidence, and evolutionary law. \n",
      "BTW- allah can do absolutely fvckall after 10 trillion prayers.\n",
      "❤️ Likes : 5\n",
      "🔁 Retweets : 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAHDP0AEAAAAADJcm6vLKu9qN8m7bgxtwlNj5ATM%3DtyXQx1eJ7FwFVZtxY8N7I9O3ZDNICMbPsNWQi6DPHy5LcWGQo0\"\n",
    "\n",
    "SEARCH_URL = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "params = {\n",
    "    \"query\": \"data science -is:retweet\",\n",
    "    \"tweet.fields\": \"created_at,public_metrics\",\n",
    "    \"max_results\": 10\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "response = requests.get(SEARCH_URL, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Affichage des tweets\n",
    "    for tweet in data.get(\"data\", []):\n",
    "        print(f\"Date : {tweet['created_at']}\")\n",
    "        print(f\"Tweet : {tweet['text']}\")\n",
    "        print(f\"Likes : {tweet['public_metrics']['like_count']}\")\n",
    "        print(f\"Retweets : {tweet['public_metrics']['retweet_count']}\")\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(f\"Erreur {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur: 400, {\"error\":{\"message\":\"(#100) Tried accessing nonexisting field (media) on node type (User)\",\"type\":\"OAuthException\",\"code\":100,\"fbtrace_id\":\"AKTwOgCdPE1OZzs76TcfsRh\"}}\n"
     ]
    }
   ],
   "source": [
    "ACCESS_TOKEN = \"EAAJrMIxe2LcBO4jyVwnHdOK8IF76i6b672M4mbAeHbCkzigACnb7I8jekZAZClUey44ABCmT5SrG8O2urtON15Q8ZAewLfLuhTV4eq32dNCn7roHxUGT8vNMRl3rR1jsJZAEaBvlSjjvtrGBcehRKBuAcec9ZBVp0YbPTWW6q2HSEZCYIsJ5HhgWoF7EaFQD4cC05XTgV89Bn0UIBEPQZDZD\"\n",
    "INSTAGRAM_ACCOUNT_ID = \"677811877933727\"\n",
    "BASE_URL = \"https://graph.facebook.com/v22.0\"\n",
    "\n",
    "def get_instagram_posts(limit=5):\n",
    "    url = f\"{BASE_URL}/{INSTAGRAM_ACCOUNT_ID}/media?fields=id,caption,like_count,comments_count,media_url,permalink,timestamp&limit={limit}&access_token={ACCESS_TOKEN}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            posts = data.get(\"data\", [])\n",
    "\n",
    "            for post in posts:\n",
    "                print(f\"Titre: {post.get('caption', 'Aucune légende')}\")\n",
    "                print(f\"Likes: {post.get('like_count', 0)}\")\n",
    "                print(f\"Commentaires: {post.get('comments_count', 0)}\")\n",
    "                print(f\"Lien du média: {post.get('media_url')}\")\n",
    "                print(f\"Lien du post: {post.get('permalink')}\")\n",
    "                print(f\"Date: {post.get('timestamp')}\")\n",
    "                print('-' * 50)\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur: {response.status_code}, {response.text}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"La requête a pris trop de temps. Vérifie ta connexion Internet ou essaie plus tard.\")\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Impossible de se connecter. Vérifie ta connexion Internet.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur est survenue: {e}\")\n",
    "\n",
    "# Exécuter la fonction\n",
    "get_instagram_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyCGwuSO-oZ2G1WMsWb7M1h3iLE91O1FH7Y'\n",
    "VIDEO_ID = 'LLAZUTbc97I'\n",
    "\n",
    "def get_video_details(video_id):\n",
    "    \n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet,statistics',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        video = response['items'][0]\n",
    "        title = video['snippet']['title']\n",
    "        description = video['snippet']['description']\n",
    "        views = video['statistics']['viewCount']\n",
    "        comment_count = video['statistics']['commentCount']\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'comment_count':  comment_count,\n",
    "            'views': views\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "video_details = get_video_details(VIDEO_ID)\n",
    "if video_details:\n",
    "    print(f\"Title: {video_details['title']}\")\n",
    "    print(f\"comment_count: {video_details['comment_count']}\")\n",
    "    print(f\"Views: {video_details['views']}\")\n",
    "else:\n",
    "    print(\"Aucune donnée trouvée pour cette vidéo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: https://www.instagram.com/sara_adjaho/\n",
      "Result 2: https://bj.linkedin.com/in/sara-odile-adjaho-b47399259\n",
      "Result 3: https://www.threads.net/@sara_adjaho\n",
      "Result 4: https://www.facebook.com/Togozik/videos/la-talentueuse-sara-adjaho-sara_adjaho-a-immortalis%C3%A9-le-concert-historique-de-sa/1608590310006912/\n",
      "Result 5: https://www.facebook.com/sara.adjaho.58/\n",
      "Result 6: https://www.tiktok.com/@sara.adjaho\n",
      "Result 7: https://www.instagram.com/ziktogo/reel/DB43tmTskBK/?locale=ne_NP&hl=af\n",
      "Result 8: https://www.linkedin.com/posts/sara-odile-adjaho-b47399259_activity-7240791877841555456-Wo0i\n",
      "Result 9: https://www.tiktok.com/@sara.adjaho/video/7280899088999140640\n",
      "Result 10: https://radioevangile66.com/author/fmevangile66/\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "def google_search(query, num_results=10):\n",
    "    try:\n",
    "       \n",
    "        search_results = search(query, num_results=num_results)\n",
    "        \n",
    "        for i, result in enumerate(search_results, start=1):\n",
    "            print(f\"Result {i}: {result}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite: {e}\")\n",
    "\n",
    "\n",
    "query = \"sara adjaho\"\n",
    "google_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: Sunday Daily Thread: What's everyone working on this week?\n",
      "Votes: 6\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jhmdi1/sunday_daily_thread_whats_everyone_working_on/\n",
      "Commentaires:\n",
      "  - Currently I’m working on another text based game. I don’t have a name for it but it’s build like an rpg. The UI is in handmade ASCII and is displayed via the console. All you would need is something like VSC to run it. The user has 9 equipment slots. Head, neck 1, neck 2, torso, gloves, pants, boots, main hand, off hand. Each one can have 0-3 affixes applies to it. The affix list for each item varies. Currently I have it sorted into a couple categories. Generic, Armor Generic, and Weapon specific. I’m still working on the gear and affix system. I’m at around 800~ and I’m almost done with the affixes. Still working on all the gear types. (Votes: 2)\n",
      "  - I'm working on a framework for local/on-prem small-data processing/ETL and warehousing.  \n",
      "\n",
      "__Background__  \n",
      "I only deal with a couple thousand rows of data between a half dozen spreadsheets each month, and my team is only me. This project was inspired by my ad-hoc scripts becoming unwieldy, but not enough to pay for anything, or to deploy a more enterprise-level solution (e.g. Dagster, Databricks, etc.)\n",
      "\n",
      "__Features__  \n",
      "It uses a decorator-API to wrap your Extract, Transform, and Load functions (which basically monkey-patch the user-defined functions as methods on the `Datasource` instances). It has support for metadata and logging, and is quite flexible. A simple example looks something like this:  \n",
      "\n",
      "\n",
      "```python\n",
      "import polars as pl\n",
      "...\n",
      "\n",
      "from my_project import Datasource\n",
      "\n",
      "my_data = Datasource(\n",
      "    name=\"MyData\",\n",
      "    filename=r\"C:/folder/*report.xlsx\",\n",
      "    ...\n",
      ")\n",
      "\n",
      "\n",
      "@my_data.extract_wrapper\n",
      "def extract(datasource, *args, **kwargs) -> pl.DataFrame:\n",
      "    df = pl.read_excel(datasource.filename, ...)\n",
      "    datasource.logger.info(\"Loaded raw data\")\n",
      "    return df  # Sets my_data.raw_data\n",
      "\n",
      "\n",
      "@my_data.transform_wrapper\n",
      "def transform(datasource, *args, **kwargs) -> pl.DataFrame \n",
      "    df = datasource.raw_data ...  # do transformations\n",
      "    return df  # Sets my_data.data\n",
      "\n",
      "\n",
      "# load function, etc.\n",
      "\n",
      "```\n",
      "\n",
      "I'm also working on the `Warehouse` object (a wrapper around a DuckDB database) that has functionality to parse SQL files for dependencies and sort them (the files) using `graphlib` before execution. I think this means that I'm employing \"DAGs\" and I am therefore hip lol.  \n",
      "\n",
      "Finally, there's a command line utility that rebuilds the database/warehouse using your `Datasource`'s, executing their ETL functions and running (sorted) SQL scripts. It can can run also run `Reports` (WIP).  \n",
      "\n",
      "Any comments are appreciated!  \n",
      "(Be critical; ClaudeAI has given me enough ego boosts lol) (Votes: 1)\n",
      "  - Using flask and dns\\_lib to create a DDNS server to resolve any hostname of a CCTV NVR (Hikvision NVRs mostly) to non static IPs. (Votes: 1)\n",
      "--------------------------------------------------\n",
      "Titre: Wednesday Daily Thread: Beginner questions\n",
      "Votes: 2\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jjyfqx/wednesday_daily_thread_beginner_questions/\n",
      "Commentaires:\n",
      "--------------------------------------------------\n",
      "Titre: excel-serializer: dump/load nested Python data to/from Excel without flattening\n",
      "Votes: 21\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jkkjvw/excelserializer_dumpload_nested_python_data/\n",
      "Commentaires:\n",
      "  - I'm interested in trying this as it's something I do all the time. (Votes: 4)\n",
      "  - Great stuff, I know just the data to test this on. (Votes: 4)\n",
      "--------------------------------------------------\n",
      "Titre: [UPDATE] safe-result 3.0: Now with Pattern Matching, Type Guards, and Way Better API Design\n",
      "Votes: 87\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jk6rex/update_saferesult_30_now_with_pattern_matching/\n",
      "Commentaires:\n",
      "  - Nice, this is way better than the previous version. (Votes: 14)\n",
      "  - Great job applying feedback. Looks a lot more ergonomic than it did a couple days ago. (Votes: 11)\n",
      "  - This is really cool. Great job getting the type inference to play nice. I'll definitely use this in future projects. (Votes: 5)\n",
      "  - Great improvement over the first iteration, this now actually provides a benefit over exceptions and feels more like the Rust implementation while still fitting into Python code. I like it!\n",
      "\n",
      "Also, strong kudos for accepting the mostly negative feedback and building on top of it. Great attitude! (Votes: 5)\n",
      "  - Maybe I’ve been stuck in Java land for too long but it would be nice if the `Result` class had `.err(…)`, `.ok(…)`, etc. static/class methods so I could shorten the import list and work just with the class to create result objects (Votes: 5)\n",
      "--------------------------------------------------\n",
      "Titre: Beesistant- a talking identification key\n",
      "Votes: 59\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jjv04p/beesistant_a_talking_identification_key/\n",
      "Commentaires:\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='MEd9rC9dQX8n5XQPsRdTPg',\n",
    "    client_secret='JT9WPr5Nvkwj7SYOG3YgFhA0LFep2A',\n",
    "    user_agent='Data science'\n",
    ")\n",
    "\n",
    "subreddit_name = 'python'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "for submission in subreddit.hot(limit=5):\n",
    "    print(f'Titre: {submission.title}')\n",
    "    print(f'Votes: {submission.score}')\n",
    "    print(f'Lien: {submission.url}')\n",
    "    print('Commentaires:')\n",
    "\n",
    "    submission.comment_sort = 'top'\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    \n",
    "    for comment in submission.comments.list()[:5]:\n",
    "        print(f'  - {comment.body} (Votes: {comment.score})')\n",
    "\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia(page_title,language='en'):\n",
    "    headers ={\n",
    "        \"User-Agent\": \"pythonscripte/1.0 (adjahosarahouefa@gmail.com)\"\n",
    "    }\n",
    "    page_wiki = wikipediaapi.Wikipedia(language, headers=headers).page(page_title)\n",
    "    \n",
    "    if not page_wiki.exists():\n",
    "        return f\"Page '{page_title}' not found.\"\n",
    "    \n",
    "    return page_wiki.text\n",
    "  \n",
    "page_title = \"Bénin\"\n",
    "content = extract_wikipedia(page_title)\n",
    "print(content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
